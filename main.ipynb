{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gymnasium as gym\n",
    "from tetris_gymnasium.envs.tetris import Tetris\n",
    "from tetris_gymnasium.mappings.rewards import RewardsMapping\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"tetris_gymnasium/Tetris\", render_mode=\"human\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    key = cv2.waitKey(100) \n",
    "env.close()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(obs, env):\n",
    "    board = obs[\"board\"]\n",
    "\n",
    "    # 1) Crop to the playable area:\n",
    "    # top 20 rows, skip the first 4 columns, skip the last 4 columns\n",
    "    cropped = board[:env.unwrapped.height, env.unwrapped.padding:-env.unwrapped.padding]\n",
    "\n",
    "    # 2) Compute column heights, holes, bumpiness, etc.\n",
    "    heights = []\n",
    "    holes = 0\n",
    "    for col_i in range(cropped.shape[1]):\n",
    "        col = cropped[:, col_i]\n",
    "        filled_idx = np.where(col > 1)[0]\n",
    "        if len(filled_idx) == 0:\n",
    "            h = 0\n",
    "        else:\n",
    "            # Height from bottom:\n",
    "            h = cropped.shape[0] - filled_idx[0]\n",
    "        heights.append(h)\n",
    "        if len(filled_idx) > 0:\n",
    "            holes_col = np.count_nonzero(col[filled_idx[0]:] == 0)\n",
    "            holes += holes_col\n",
    "\n",
    "    bumpiness = sum(abs(heights[i] - heights[i+1]) for i in range(len(heights)-1))\n",
    "    total_height = sum(heights)\n",
    "\n",
    "    return (tuple(heights), holes, bumpiness, total_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(object):\n",
    "    def __init__(self, env, alpha, epsilon, gamma, timeout):\n",
    "        # definte the parameters\n",
    "        self.alpha = alpha \n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.timeout = timeout\n",
    "\n",
    "        # environment\n",
    "        self.env = env\n",
    "\n",
    "        # define the Q value table\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    def behavior_policy(self, state):\n",
    "\n",
    "        # explore\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        \n",
    "        # new state\n",
    "        if state not in self.Q or not self.Q[state]:\n",
    "            return self.env.action_space.sample()\n",
    "        \n",
    "        # exploit\n",
    "        return max(self.Q[state], key=self.Q[state].get)\n",
    "        \n",
    "    def update(self, s, a, r, s_prime, a_prime):\n",
    "\n",
    "        # update the Q value table using the SARSA update rule\n",
    "        self.Q[s][a] += self.alpha * (r + self.gamma * self.Q[s_prime][a_prime] - self.Q[s][a])\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "\n",
    "        # initialize rewards\n",
    "        rewards = []\n",
    "\n",
    "        # train the agent for a number of episodes\n",
    "        for episode in range(num_episodes):\n",
    "\n",
    "            # epsilon decay\n",
    "            # self.epsilon = max(0.01, self.epsilon * 0.99)\n",
    "\n",
    "            # reset the environment\n",
    "            obs, info = self.env.reset()\n",
    "            state = extract_features(obs, self.env)\n",
    "\n",
    "            # get the first action\n",
    "            action = self.behavior_policy(state)\n",
    "\n",
    "            # initialize the reward\n",
    "            episode_reward = 0\n",
    "\n",
    "            for t in range(self.timeout):\n",
    "\n",
    "                # render the environment\n",
    "                # self.env.render()\n",
    "                # key = cv2.waitKey(100)\n",
    "\n",
    "                # take the action\n",
    "                next_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "                next_state = extract_features(next_obs, self.env)\n",
    "\n",
    "                # get the next action\n",
    "                next_action = self.behavior_policy(next_state)\n",
    "\n",
    "                # update the Q value table\n",
    "                self.update(state, action, reward, next_state, next_action)\n",
    "\n",
    "                # update the state, action, and reward\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                episode_reward += reward\n",
    "\n",
    "                # if the game is over, reset the environment\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "            # append the reward to the list of rewards\n",
    "            rewards.append(episode_reward)\n",
    "\n",
    "            # print the average reward every 100 episodes\n",
    "            if episode % 100 == 0:\n",
    "                avg_reward = np.mean(rewards[-100:])\n",
    "                print(f\"Episode {episode}, Average Reward: {avg_reward}\")\n",
    "\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Average Reward: 0.0\n",
      "Episode 100, Average Reward: 0.46\n",
      "Episode 200, Average Reward: 0.04\n",
      "Episode 300, Average Reward: -0.06\n",
      "Episode 400, Average Reward: -0.14\n",
      "Episode 500, Average Reward: -0.5\n",
      "Episode 600, Average Reward: -0.04\n",
      "Episode 700, Average Reward: 0.02\n",
      "Episode 800, Average Reward: -0.36\n",
      "Episode 900, Average Reward: -0.16\n",
      "Episode 1000, Average Reward: -0.28\n",
      "Episode 1100, Average Reward: -0.12\n",
      "Episode 1200, Average Reward: -0.28\n",
      "Episode 1300, Average Reward: -0.18\n",
      "Episode 1400, Average Reward: -0.4\n",
      "Episode 1500, Average Reward: -0.26\n",
      "Episode 1600, Average Reward: -0.3\n",
      "Episode 1700, Average Reward: -0.44\n",
      "Episode 1800, Average Reward: 0.02\n",
      "Episode 1900, Average Reward: -0.22\n",
      "Episode 2000, Average Reward: -0.22\n",
      "Episode 2100, Average Reward: -0.7\n",
      "Episode 2200, Average Reward: -0.6\n",
      "Episode 2300, Average Reward: -0.48\n",
      "Episode 2400, Average Reward: -0.1\n",
      "Episode 2500, Average Reward: -0.76\n",
      "Episode 2600, Average Reward: -0.42\n",
      "Episode 2700, Average Reward: -0.3\n",
      "Episode 2800, Average Reward: -0.46\n",
      "Episode 2900, Average Reward: -0.76\n",
      "Episode 3000, Average Reward: -0.14\n",
      "Episode 3100, Average Reward: -0.32\n",
      "Episode 3200, Average Reward: -0.7\n",
      "Episode 3300, Average Reward: -0.5\n",
      "Episode 3400, Average Reward: -0.48\n",
      "Episode 3500, Average Reward: -0.66\n",
      "Episode 3600, Average Reward: -0.46\n",
      "Episode 3700, Average Reward: -0.58\n",
      "Episode 3800, Average Reward: -0.08\n",
      "Episode 3900, Average Reward: -0.7\n",
      "Episode 4000, Average Reward: -0.28\n",
      "Episode 4100, Average Reward: -0.4\n",
      "Episode 4200, Average Reward: -0.48\n",
      "Episode 4300, Average Reward: -0.5\n",
      "Episode 4400, Average Reward: -0.64\n",
      "Episode 4500, Average Reward: -0.18\n",
      "Episode 4600, Average Reward: -0.44\n",
      "Episode 4700, Average Reward: -0.58\n",
      "Episode 4800, Average Reward: -0.64\n",
      "Episode 4900, Average Reward: -0.3\n",
      "Episode 5000, Average Reward: -0.4\n",
      "Episode 5100, Average Reward: -0.44\n",
      "Episode 5200, Average Reward: -0.54\n",
      "Episode 5300, Average Reward: -0.72\n",
      "Episode 5400, Average Reward: -0.54\n",
      "Episode 5500, Average Reward: -0.58\n",
      "Episode 5600, Average Reward: -0.16\n",
      "Episode 5700, Average Reward: -0.44\n",
      "Episode 5800, Average Reward: -0.58\n",
      "Episode 5900, Average Reward: -0.46\n",
      "Episode 6000, Average Reward: -0.82\n",
      "Episode 6100, Average Reward: -0.06\n",
      "Episode 6200, Average Reward: -0.36\n",
      "Episode 6300, Average Reward: -0.58\n",
      "Episode 6400, Average Reward: -0.4\n",
      "Episode 6500, Average Reward: -0.22\n",
      "Episode 6600, Average Reward: -0.58\n",
      "Episode 6700, Average Reward: -0.06\n",
      "Episode 6800, Average Reward: -0.26\n",
      "Episode 6900, Average Reward: -0.28\n",
      "Episode 7000, Average Reward: -0.42\n",
      "Episode 7100, Average Reward: -0.86\n",
      "Episode 7200, Average Reward: -0.26\n",
      "Episode 7300, Average Reward: 0.0\n",
      "Episode 7400, Average Reward: -0.62\n",
      "Episode 7500, Average Reward: -0.3\n",
      "Episode 7600, Average Reward: -0.44\n",
      "Episode 7700, Average Reward: -0.28\n",
      "Episode 7800, Average Reward: -0.44\n",
      "Episode 7900, Average Reward: -0.54\n",
      "Episode 8000, Average Reward: -0.16\n",
      "Episode 8100, Average Reward: -0.76\n",
      "Episode 8200, Average Reward: -0.22\n",
      "Episode 8300, Average Reward: -0.32\n",
      "Episode 8400, Average Reward: -0.44\n",
      "Episode 8500, Average Reward: -0.7\n",
      "Episode 8600, Average Reward: -0.36\n",
      "Episode 8700, Average Reward: -0.64\n",
      "Episode 8800, Average Reward: -0.34\n",
      "Episode 8900, Average Reward: -0.62\n",
      "Episode 9000, Average Reward: -0.26\n",
      "Episode 9100, Average Reward: -0.08\n",
      "Episode 9200, Average Reward: -0.12\n",
      "Episode 9300, Average Reward: -0.46\n",
      "Episode 9400, Average Reward: -0.08\n",
      "Episode 9500, Average Reward: -0.28\n",
      "Episode 9600, Average Reward: -0.4\n",
      "Episode 9700, Average Reward: 0.0\n",
      "Episode 9800, Average Reward: -0.32\n",
      "Episode 9900, Average Reward: -0.42\n"
     ]
    }
   ],
   "source": [
    "rewards = RewardsMapping(\n",
    "    alife=2.0,\n",
    "    clear_line=1.0,\n",
    "    game_over=-20.0,\n",
    "    invalid_action=-0.1,\n",
    ")\n",
    "\n",
    "env = gym.make(\"tetris_gymnasium/Tetris\", render_mode=\"human\", rewards_mapping=rewards)\n",
    "agent = SARSA(env, alpha=0.1, epsilon=0.1, gamma=0.99, timeout=1000)\n",
    "\n",
    "agent.train(num_episodes=10000)\n",
    "\n",
    "env.close()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
